{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This memory describes the steps followed for the preparation of the Kschool Data Science Master 15th Edition TFM: 'Test your business viability' as well as the main results\n",
    "\n",
    "Some **background**   \n",
    "When I started to look for an idea for the project, I came into a open dataset of Madrid city council that contains information about the different retail stores and activities licensed in Madrid since 2014. See references in: https://datos.madrid.es/sites/v/index.jsp?vgnextoid=66665cde99be2410VgnVCM1000000b205a0aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD   \n",
    "This data reminded me about the feeling that I had, that some people open their business without a business case analysis and many of them close in a short period of time. So I decided to analyse the commercial premises census evolution and look for some pattern.\n",
    "\n",
    "The **TFM objective**, the main goal of this TFM is to use this information as a base for an advanced analytics model that predicts the probability that a commercial premises will be closed in a 3 years timeframe.   \n",
    "This is a supervised classification problem and the preparation of this project has been full of of Pandas and all kind of classifications algorithms and recommendations to get the most of them (I have added some references at the end of the document). \n",
    "\n",
    "Some **important decisions:**   \n",
    "During the project preparation and after some time working with the dataset, I realized that the quality of the data was not good enough. I made some on the field research and Google searchs and confirmed that the 'desc_situacion_local' that indicates the status of a store ('open', closed', etc...) was not up to date.   \n",
    "I decided to go on with the information available in what has become an exercise of learning and investigation that has allowed me to apply most of the steps and concepts learnt during the master. Main assumptions taken:\n",
    "- **About Data sources**: 2014 files had a different structure, with different identifiers and tags so I decided to use the information available since 2015 till september 2019.  \n",
    "\n",
    "\n",
    "- **About Target variable**: I wanted to analyze the probability of a business to open and close in a 2 years timeframe but I had not enough samples (less than 1% over the total population). Finally, I defined my target variable as **commercial premises that closed between 2017 to the date (3 years timeframe)**.\n",
    "\n",
    "\n",
    "- **Variables**: I have not found good predictive variables. I started with the retail stores files with no results so I added additional features trying to get some results.  \n",
    "I have used information of Madrid population census also available in Madrid Opendata Portal and some information about floating population in the different districts of Madrid during one week of April 2018, kindly provided by Kinneo\n",
    "\n",
    "\n",
    "- **About the Models**: I have used Logistic Regression as a baseline. \n",
    "I started testing KNN, Decission Tree, Random Forest and XGboost. The results were quite similar and far from good (aroung 60% AUC) so I focused on Random Forest and XGboost that got better results.  \n",
    "For the sake of simplicity, I will only show the analysis performed with Logistic Regression, Random Forest and XGboost.  \n",
    "I will deliver the report of the modeling results in a Jupyter Notebook \n",
    "\n",
    "- **Metrics**: the main metrics are Recall, AUC and f1 score. I have also analysed lift Precision and Recall curve and Roc curve. \n",
    "\n",
    "\n",
    "- **Project structure and presentation**:\n",
    "The result and insights of the project are presented in Jupyter notebooks for code and visual reproductibility   \n",
    "\n",
    "I have followed the different steps of a classification problems:   \n",
    "- Data gathering\n",
    "- Data preparation: cleaning, target generation, new variables generation\n",
    "- Exploratory data analysis\n",
    "- Data modeling\n",
    "- Testing and metrics  \n",
    "\n",
    "It is possible to execute the results through the Notebooks below. \n",
    "\n",
    "**Steps for navigating through the contents of the project**: \n",
    "- **First**: check libraries and install \n",
    "- **Second**: execute jupyter notebook Data_loading_and_preparation\n",
    "- **Third**: execute jupyter notebook Classification Modeling\n",
    "- **Appendix**\n",
    "\n",
    "**Important notes**\n",
    "- Please, \"Clone and Download\" the .git from [TFM Github repository](./https://github.com/bdiazcor/TFM-Test-your-business-viability) to local PC and execute as instructions in this readme. Do not change the directories tree structure.\n",
    "- I suggest to run the project from the readme. However, the different notebooks can also be launched directly from folder TBV_v1. It is important to follow the steps listed above.\n",
    "- I have added a Java script code to hide the raw code at the beginning of the notebooks and facilitate the reading. You can toggle on it to expand the code. \n",
    "\n",
    "For example, the following notebook will be as follows after edition\n",
    "![caption](button.png)\n",
    "\n",
    "And after Run All, only the output will be shown and there will be a buttom at the top to expand the code if needed\n",
    "![caption](output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TFM modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Install libraries\n",
    "\n",
    "Recommended installing all the libraries using the **Anaconda Distribution**.\n",
    "First, download [Anaconda](./https://www.anaconda.com/distribution/). \n",
    "Second, install the version of Anaconda which you downloaded, following the instructions on the download page\n",
    "\n",
    "I had almost all the libraries available. New libraries I had to install:\n",
    "- **scipy 1.1.0**: > pip install scipy\n",
    "- **scikitplot 0.3.7**: > pip install scikit-plot\n",
    "- **imblearn 0.5.0**: > pip install imbalanced-learn\n",
    "- **geopy 1.20.0**: > pip install geopy\n",
    "- **utm 0.5.0**: > pip install utm\n",
    "\n",
    "Full list of libraries used for this TFM:\n",
    "- **Python 3.7.3**   \n",
    "- **pip 19.1.1**   \n",
    "- **Jupyter Notebook 6.0.2**   \n",
    "- **Pandas 0.23.4**   \n",
    "- **Numpy 1.15.4**    \n",
    "- **matplotlib 3.0.2**      \n",
    "- **sklearn 0.21.3**    \n",
    "- **pickle**: part of the standar library of Python\n",
    "- **seaborn 0.9.0**    \n",
    "- **xgboost 0.90**   \n",
    "- **scipy 1.1.0**    \n",
    "- **scikitplot 0.3.7**   \n",
    "- **imblearn 0.5.0**  \n",
    "- **geopy 1.20.0** \n",
    "- **utm 0.5.0** \n",
    "\n",
    "Alternative to Anaconda, list of commands to execute (once Python 3 and Pypi installed):   \n",
    "python get-pip.py   \n",
    "pip install jupyter   \n",
    "pip install pandas   \n",
    "pip install numpy   \n",
    "pip install matplotlib   \n",
    "pip install -U scikit-learn   \n",
    "pip install seaborn   \n",
    "pip install xgboost   \n",
    "pip install scipy   \n",
    "pip install scikit-plot   \n",
    "pip install imbalanced-learn   \n",
    "pip install geopy   \n",
    "pip install utm   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Data loading and data preparation\n",
    "\n",
    "[Launches Data loading and preparation Notebook](./TBV1_data_cleaning.ipynb). Follows the structure:\n",
    "\n",
    "**1. Data gathering**   \n",
    "     1) Activities files (from 2015 - 2019, Ayuntamiento Madrid)   \n",
    "     2) Madrid population database (1st january 2019, Ayuntamiento Madrid)   \n",
    "     3) Madrid floating population (16-22 april 2018, Private source)    \n",
    "\n",
    "**2. Data preparation**: cleaning, target generation, new variables generation   \n",
    "     1) Commercial premises status normalization   \n",
    "     2) NaN management   \n",
    "     3) Merge all years commercial premises info in a single DataFrame   \n",
    "     4) Generate target variable   \n",
    "     5) Standardize type of activities   \n",
    "     6) Generate interesting variables and convert UTM coordinates   \n",
    "     7) Add population info   \n",
    "     8) Select activities for analysis   \n",
    "     9) Merge with info points in radius   \n",
    "     10) Export to csv   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Classification Modeling\n",
    "\n",
    "[Launches Data modeling Notebook](./TBV1_classification_model.ipynb#). Follows the structure:   \n",
    "\n",
    "**1. Import libraries**\n",
    "\n",
    "\n",
    "**2. Exploratory Data Analysis**   \n",
    "1) Load .csv to DataFrame.    \n",
    "2) Select columns of interest      \n",
    "3) Identify type of columns   \n",
    "4) Dummify categorical values   \n",
    "5) train and test split    \n",
    "\n",
    "**3. Modeling**   \n",
    "1) Base model: Logistic Regression    \n",
    "2) Random Forest   \n",
    "3) XGboost. Includes optimal cut-off analysis and cummulative Gain and lift charts   \n",
    "4) Features importance with the best estimator   \n",
    "5) Future prediction (with reserved 5% of samples)    \n",
    "\n",
    "**4. Summary and conclusions**   \n",
    "1) Conclusions   \n",
    "2) Next steps   \n",
    "3) Summary of the exercise   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Next steps\n",
    "Upgrades with future deliverables:\n",
    "- Rethink and **correct the target variable**\n",
    "- **Fine tune the models** and hyperparameters\n",
    "- Get **more features**: \n",
    "    - New: location vs points of interest and transport stations; web scrapping in Google places for more accurate information about commercial premises status); stores prices from Idealista\n",
    "    - More granularity of the existing (neighbourhood or postal code level)    \n",
    "- **Pilot** the results. This could be slow. I will look for some comditions to test quickly: ie. visit the premises with the highest probability of closure a validate the context with the data available. \n",
    "- **Code optimization**\n",
    "- **Industrialize** all the steps in a pipeline\n",
    "- **End user web or app** for information queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Appendix:\n",
    "\n",
    "### A) Data\n",
    "All the datasets needed to run the project are in the folder /TBV_v1/Data. Please, \"Clone and Download\" the folder TBV_v1 from [TFM Github repository](./https://github.com/bdiazcor/TFM-Test-your-business-viability) to local PC and execute the different parts from this document from here. Do not change the directories structure locally.\n",
    "\n",
    "### B) Support Notebooks and scripts \n",
    "- [Points_in_radius](./Points_in_radius.ipynb): calculates the number of retail stores of the same category in a radius. It execution takes time (40 min) so the result is already available to merge with the rest of information\n",
    "- [clean_functions](./clean_functions.py): set of function to simplify notebooks and reproduce code \n",
    "- [dataset_for_modeling](./dataset_for_modeling.py): it't a simplified version of the chapter Exploratory Data Analysis to generate the dataset for modeling in other notebook (Fine tuning Random Forest and Fine tuning XGboost)\n",
    "- [Future_prediction](./Future_predictions.py): it is a function that returns information about the commercial premises predicted. \n",
    "- [Metrics](./metrics.py): a script that included different functions to plot Roc and Precision and Recall Curve and calculates the optimal cut-off\n",
    "- [Random Forest Tuning](./TBV1_clas_rf.ipynb): code extract prepared to be executed independently of Classification Modeling Notebook. The results have been already included in Classification Modeling Notebook\n",
    "- [Xgboost tuning](./TBV1_cla_xgb.ipynb): code extract prepared to be executed independently of Classification Modeling Notebook. The results have been already included in Classification Modeling Notebook\n",
    "\n",
    "### C) Variables dictionary\n",
    "- [Dictionary of variables](./dictionary.pdf)\n",
    "\n",
    "### D) References\n",
    "\n",
    "- Madrid council open portal (retail stores and activities census): https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=23160329ff639410VgnVCM2000000c205a0aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextfmt=default\n",
    "\n",
    "- Madrid council open portal (population census): https://datos.madrid.es/portal/site/egob/menuitem.c05c1f754a33a9fbe4b2e4b284f1a5a0/?vgnextoid=1d755cde99be2410VgnVCM1000000b205a0aRCRD&vgnextchannel=374512b9ace9f310VgnVCM100000171f5a0aRCRD&vgnextfmt=default\n",
    "\n",
    "- Description of atributes in commercial premises datasets: https://datos.madrid.es/FWProjects/egob/Catalogo/Economia/Ficheros/Estructura_DS_FicheroCLA.pdf\n",
    "\n",
    "- Incomes per district: https://www.expansion.com/economia/2019/09/12/5d7a1c78e5fdea4b218b458e.html\n",
    "\n",
    "- EDA analysis and features transformation: https://medium.com/vickdata/four-feature-types-and-how-to-transform-them-for-machine-learning-8693e1c24e80 \n",
    "\n",
    "- Features encoding: https://towardsdatascience.com/an-easier-way-to-encode-categorical-features-d840ff6b3900  \n",
    "\n",
    "- Roc and precision and recall curves: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/ \n",
    "\n",
    "- Fine tuning a classifier with Gridsearch: https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65\n",
    "\n",
    "- Xgboost fine tuning: https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "\n",
    "- Xgboost tutorial: https://www.datacamp.com/community/tutorials/xgboost-in-python\n",
    "\n",
    "- Optimal cut-off point: https://stackoverflow.com/questions/28719067/roc-curve-and-cut-off-point-python\n",
    "\n",
    "- Feature importance: https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/\n",
    "\n",
    "- Cummulative gain and lift curves: https://www.datavedas.com/model-evaluation-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
